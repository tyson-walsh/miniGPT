apiVersion: v1
kind: Pod
metadata:
  name: minigpt-lora-pod
  namespace: thwalsh
spec:
  securityContext:
    runAsUser: 264391
    runAsGroup: 1132
  restartPolicy: Never
  nodeSelector:
    nvidia.com/gpu.product: NVIDIA-A40
  volumes:
    - name: home-vol
      nfs:
        server: 128.239.56.166
        path: /sciclone/home/thwalsh
  containers:
    - name: minigpt-lora-container
      image: "ghcr.io/tyson-walsh/minigpt:latest"
      imagePullPolicy: Always
      resources:
        requests:
          memory: "64Gi"
          cpu: "8"
          nvidia.com/gpu: "2"
        limits:
          memory: "64Gi"
          cpu: "8"
          nvidia.com/gpu: "2"
      volumeMounts:
        - name: home-vol
          mountPath: /sciclone/home/thwalsh
      workingDir: /sciclone/home/thwalsh/GenAI
      env:
        - name: DEV_MODE
          value: "false"        # "true" for dev mode (outputs to models_debug), "false" for full mode (outputs to k8s_models)
        - name: EPOCHS
          value: "3"
        - name: BATCH_SIZE
          value: "64"
        # - name: TRAIN_BATCH_LIMIT
        #   value: "10000"
        # - name: VAL_BATCH_LIMIT
        #   value: "1000"
        - name: SAVE_PATH
          value: "k8s_models"  # Overridden by dev_mode in train.py
        - name: TRANSFORMERS_CACHE
          value: "/sciclone/home/thwalsh/GenAI/.cache/huggingface"
        - name: OMP_NUM_THREADS
          value: "4"
        # VARIANTS are listed in a block scalar. 
        # To run all 7 variants, keep them all:
        #   base, lora, lora_plus, prefix_default, prefix_army_football, bias, adapter
        # To run only one or a subset, remove lines you don't want. 
        - name: VARIANTS
          value: |-
            lora
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e
          echo "------------------------------------------------------------"
          echo "Verifying environment and GPU setup..."
          echo "------------------------------------------------------------"
          python --version
          python -m pip --version
          nvidia-smi || echo "nvidia-smi not found"
          python -c "import torch; print('PyTorch Version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('cuDNN available:', torch.backends.cudnn.is_available())"

          echo "------------------------------------------------------------"
          echo "Creating output directories..."
          mkdir -p k8s_models
          mkdir -p models_debug

          echo "------------------------------------------------------------"
          echo "Running training with 2 GPUs!"
          echo "DEV_MODE=$DEV_MODE"
          echo "Selected variants:"
          echo "$VARIANTS"
          echo "Logs will be written to miniGPT_k8s_lora_debug.log."
          echo "------------------------------------------------------------"

          # Wrap $VARIANTS in quotes to ensure it's treated as one argument
          python -m torch.distributed.run --nproc_per_node=2 train.py \
            --data_path data \
            --variants "$VARIANTS" > miniGPT_k8s_lora_debug.log 2>&1

          echo "------------------------------------------------------------"
          echo "Training completed. Check miniGPT_k8s_lora_debug.log for details."
          echo "------------------------------------------------------------"
